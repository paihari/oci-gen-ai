# Self-Hosted GenAI Code with RAG Features

# AI 101

Embarking on a new topic can be daunting. It helps to have some foundational knowledge or pointers to give a sense of belonging and calmness while navigating the complexities. This was my initial approach to AI, and over time, my understanding has evolved, adding new insights and refining the sequence of concepts.

![AI 101](docs/20240312-AI-101.drawio.png)



# Large Language Models (LLMs)

## Key Points

- **Processing Text with LLMs**:
  - LLMs use encoders and decoders to convert words into numerical vectors.
  - They capture the semantics of text and generate text token by token.

- **Architectures and Tasks**:
  - LLM architectures include encoder, decoder, and encoder-decoder models.
  - These architectures are suited to various tasks such as translation, summarization, and question-answering.

- **Prompting Techniques**:
  - Techniques like in-context learning and chain-of-thought prompting are used to improve LLM performance.
  - Prompting can introduce issues like prompt injection.

- **Hardware Requirements**:
  - LLMs require substantial hardware resources for training and inference.
  - Advanced models like GPT-4 need high-performance GPUs.

- **Applications**:
  - Code models (e.g., Copilot, Codex) and multi-modal models handle text, image, and audio data.
  - Language agents can reason, plan, and take actions, such as those in ReAct and Toolformer.


![Fundamentals Diagram](docs/20240312-AI-Fundementals.drawio.png)


# Generative AI Services

## Key Points

- **Pre-trained Foundational Models**:
  - Used for text generation, summarization, and embedding.
  - Models like Cohere and Llama are commonly used.

- **Generation and Embedding Services**:
  - Generate vectors for words and sentences to determine semantic similarity.
  - Utilize models for various applications, including semantic similarity.

- **Prompt Engineering**:
  - Essential for refining AI outputs.
  - Techniques like in-context learning and K-shot prompting enhance performance.

- **Customization with User Data**:
  - Involves prompt engineering, retrieval-augmented generation (RAG), and fine-tuning.
  - Allows for tailored AI solutions.

- **Inference Services**:
  - Facilitate deployment of customized models.
  - Provide tailored responses based on specific prompts and data.


![Gen AI Services Diagram](docs/20240312-AI-Generative-AI-Service.drawio.png)


# Retrieval-Augmented Generation (RAG)

RAG (Retrieval-Augmented Generation) is an advanced natural language processing (NLP) model designed to enhance text generation by incorporating relevant information from external sources. This README explains the basics of RAG, its core components, the concepts of RAG Sequence and RAG Token Types, and outlines the RAG pipeline.

## Basics of RAG

RAG improves the quality and relevance of generated responses by combining three main components:
1. **Retriever**
2. **Ranker**
3. **Generator**

## Components of RAG

### 1. Retriever
- **Function:** Searches a large corpus of documents or a knowledge base to find relevant information based on the input query.
- **Implementation:** Utilizes dense retrieval methods (like DPR) or traditional sparse retrieval methods (like BM25).

### 2. Ranker
- **Function:** Ranks the retrieved documents or passages based on their relevance to the query.
- **Purpose:** Enhances the quality of the information fed into the generator.
- **Mechanism:** Uses scoring systems or additional models to assess relevance.

### 3. Generator
- **Function:** Generates a coherent and contextually relevant response using the ranked documents or passages.
- **Implementation:** Often uses pre-trained language models such as BERT, GPT, or T5.
- **Outcome:** Ensures the final output is both informative and appropriate to the context.

## RAG Sequence and RAG Token Types

### RAG Sequence
- Describes the process where the retriever finds relevant documents, the ranker orders them, and the generator produces the final output.
- Enhances response context and accuracy by leveraging external knowledge.

### RAG Token Types
- **Input Tokens:** Initial query or prompt provided to the model.
- **Retrieved Document Tokens:** Tokens from documents retrieved by the retriever.
- **Generated Response Tokens:** Tokens in the final output generated by the model.

## RAG Pipeline: Ingestion, Retrieval, and Generation

### 1. Ingestion
- **Purpose:** Prepares and indexes a large corpus of documents for efficient retrieval.
- **Techniques:** Includes tokenization, embedding creation, and storage of embeddings.

### 2. Retrieval
- **Function:** The retriever searches the indexed corpus for relevant documents based on the input query.
- **Techniques:** Can involve dense retrieval (using neural networks) or sparse retrieval (using inverted indexes).

### 3. Generation
- **Function:** The generator takes the top-ranked documents from the retriever to produce a response.
- **Outcome:** Ensures the response is coherent, contextually appropriate, and informative.
- **Balance:** Utilizes retrieved information while generating new content based on the modelâ€™s training.


![RAG Diagram](docs/20240312-AI-RAG.drawio.png)
